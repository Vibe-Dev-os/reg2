# ğŸš€ Deploy Backend to Render

Complete guide to deploy your FastAPI backend to Render.

---

## ğŸ“‹ Prerequisites

1. âœ… GitHub account
2. âœ… Render account (free) - [Sign up here](https://render.com)
3. âœ… Your code pushed to GitHub

---

## ğŸ”§ Step 1: Prepare Your Repository

### 1.1 Push Backend to GitHub

If you haven't already, push your code to GitHub:

```bash
cd c:\xampp\htdocs\machine\machine-learning
git init
git add .
git commit -m "Initial commit - Student Grade Predictor"
git branch -M main
git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git
git push -u origin main
```

### 1.2 Verify Files

Make sure these files exist in your `backend/` folder:
- âœ… `main.py` - Your FastAPI application
- âœ… `requirements.txt` - Python dependencies
- âœ… `render.yaml` - Render configuration (just created)
- âœ… `student-por.csv` - Dataset (in parent directory)

---

## ğŸŒ Step 2: Deploy to Render

### Option A: Using render.yaml (Recommended)

1. **Go to Render Dashboard**
   - Visit [https://dashboard.render.com](https://dashboard.render.com)
   - Sign in or create account

2. **Create New Web Service**
   - Click "New +" â†’ "Web Service"
   - Connect your GitHub repository
   - Select your repository

3. **Configure Service**
   - **Name:** `student-grade-predictor-api`
   - **Region:** Oregon (US West) or closest to you
   - **Branch:** `main`
   - **Root Directory:** `backend`
   - **Runtime:** Python 3
   - **Build Command:** `pip install -r requirements.txt`
   - **Start Command:** `uvicorn main:app --host 0.0.0.0 --port $PORT`

4. **Environment Variables** (Optional)
   - Click "Advanced"
   - Add if needed:
     - `PYTHON_VERSION`: `3.12.0`

5. **Deploy**
   - Click "Create Web Service"
   - Wait 5-10 minutes for deployment

### Option B: Manual Configuration

If render.yaml doesn't work, use these settings:

**Service Settings:**
```
Name: student-grade-predictor-api
Environment: Python 3
Region: Oregon (US West)
Branch: main
Root Directory: backend
Build Command: pip install -r requirements.txt
Start Command: uvicorn main:app --host 0.0.0.0 --port $PORT
```

**Advanced Settings:**
```
Auto-Deploy: Yes
Health Check Path: /
```

---

## ğŸ“ Step 3: Configure Build Settings

### Build Command
```bash
pip install -r requirements.txt
```

### Start Command
```bash
uvicorn main:app --host 0.0.0.0 --port $PORT
```

**Important:** Render automatically sets the `$PORT` environment variable.

---

## ğŸ” Step 4: Verify Deployment

### 4.1 Check Deployment Status

In Render dashboard:
- âœ… Build logs should show successful installation
- âœ… Service should show "Live" status
- âœ… You'll get a URL like: `https://student-grade-predictor-api.onrender.com`

### 4.2 Test API Endpoints

Open your browser or use curl:

**Test Root Endpoint:**
```bash
curl https://YOUR_APP_NAME.onrender.com/
```

Expected response:
```json
{
  "message": "Student Grade Prediction API",
  "status": "running"
}
```

**Test Docs:**
Visit: `https://YOUR_APP_NAME.onrender.com/docs`

**Test Model Status:**
```bash
curl https://YOUR_APP_NAME.onrender.com/model/status
```

---

## ğŸ¯ Step 5: Train the Model on Render

### Important: Dataset Location

The model needs `student-por.csv`. Make sure it's in the correct location:

**Option 1: Include in Repository**
```
machine-learning/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ student-por.csv  â† Dataset here
```

Update `main.py` line 173:
```python
data_path = os.path.join(os.path.dirname(__file__), '..', 'student-por.csv')
```

**Option 2: Upload via API** (if dataset is large)
Create an upload endpoint or use environment variables.

### Train the Model

Once deployed, train the model:

```bash
curl -X POST https://YOUR_APP_NAME.onrender.com/train
```

This will:
- Load the dataset
- Apply all improvements (outlier removal, feature selection, etc.)
- Train the Random Forest model
- Save model artifacts

**Note:** First training might take 1-2 minutes.

---

## ğŸ”— Step 6: Update Frontend to Use Render API

### Update API Base URL

In your frontend `lib/api.ts`:

```typescript
const API_BASE_URL = process.env.NEXT_PUBLIC_API_URL || 
                     "https://YOUR_APP_NAME.onrender.com";
```

Or create `.env.local`:
```
NEXT_PUBLIC_API_URL=https://YOUR_APP_NAME.onrender.com
```

---

## âš™ï¸ Step 7: Environment Variables (Optional)

If you need to add environment variables:

1. Go to Render Dashboard
2. Select your service
3. Click "Environment"
4. Add variables:
   - `PYTHON_VERSION`: `3.12.0`
   - `CORS_ORIGINS`: Your frontend URL

---

## ğŸ› Troubleshooting

### Issue 1: Build Fails

**Error:** `ModuleNotFoundError`

**Solution:**
- Check `requirements.txt` has all dependencies
- Verify Python version compatibility

### Issue 2: Application Crashes

**Error:** `Address already in use`

**Solution:**
- Ensure start command uses `$PORT` variable:
  ```bash
  uvicorn main:app --host 0.0.0.0 --port $PORT
  ```

### Issue 3: CORS Errors

**Error:** `CORS policy: No 'Access-Control-Allow-Origin'`

**Solution:**
- Update CORS origins in `main.py` (already done)
- Add your frontend URL to allowed origins

### Issue 4: Model Not Found

**Error:** `Model not trained`

**Solution:**
- Train the model via `/train` endpoint
- Ensure `student-por.csv` is accessible
- Check file paths in code

### Issue 5: Slow Cold Starts

**Issue:** First request takes 30+ seconds

**Explanation:**
- Render free tier spins down after 15 minutes of inactivity
- First request "wakes up" the service

**Solutions:**
- Upgrade to paid tier ($7/month) for always-on
- Use a cron job to ping every 10 minutes
- Accept the cold start on free tier

---

## ğŸ“Š Monitoring

### View Logs

In Render Dashboard:
1. Select your service
2. Click "Logs" tab
3. See real-time application logs

### Check Metrics

Monitor:
- âœ… CPU usage
- âœ… Memory usage
- âœ… Request count
- âœ… Response times

---

## ğŸ’° Pricing

### Free Tier
- âœ… 750 hours/month
- âœ… Automatic sleep after 15 min inactivity
- âœ… 512 MB RAM
- âœ… 0.1 CPU
- âš ï¸ Cold starts (30s delay)

### Starter Tier ($7/month)
- âœ… Always on (no cold starts)
- âœ… 512 MB RAM
- âœ… 0.5 CPU
- âœ… Better performance

---

## ğŸ”’ Security Best Practices

### 1. Restrict CORS Origins

After deployment, update `main.py`:

```python
allow_origins=[
    "https://your-frontend-domain.vercel.app",
    "http://localhost:3000",  # For local development
]
```

### 2. Add Rate Limiting

Install:
```bash
pip install slowapi
```

Add to `main.py`:
```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.post("/predict")
@limiter.limit("10/minute")
async def predict_grade(request: Request, student: StudentData):
    # ... existing code
```

### 3. Environment Variables

Don't hardcode sensitive data. Use Render's environment variables.

---

## ğŸš€ Deployment Checklist

Before deploying:

- [ ] Code pushed to GitHub
- [ ] `requirements.txt` is complete
- [ ] `render.yaml` is configured
- [ ] CORS origins updated
- [ ] Dataset is accessible
- [ ] Start command uses `$PORT`
- [ ] Tested locally

After deploying:

- [ ] Service shows "Live" status
- [ ] API root endpoint works
- [ ] `/docs` page loads
- [ ] Model trained successfully
- [ ] Frontend can connect
- [ ] Predictions work

---

## ğŸ“š Useful Commands

### Check Service Status
```bash
curl https://YOUR_APP_NAME.onrender.com/
```

### Train Model
```bash
curl -X POST https://YOUR_APP_NAME.onrender.com/train
```

### Get Model Status
```bash
curl https://YOUR_APP_NAME.onrender.com/model/status
```

### Make Prediction
```bash
curl -X POST https://YOUR_APP_NAME.onrender.com/predict \
  -H "Content-Type: application/json" \
  -d @sample_student.json
```

---

## ğŸ‰ Success!

Your backend is now deployed! You should have:

âœ… Live API at `https://YOUR_APP_NAME.onrender.com`  
âœ… Interactive docs at `https://YOUR_APP_NAME.onrender.com/docs`  
âœ… Trained model ready for predictions  
âœ… Auto-deploy on git push  

---

## ğŸ”— Next Steps

1. **Deploy Frontend** to Vercel/Netlify
2. **Update API URL** in frontend
3. **Test end-to-end** functionality
4. **Monitor performance** in Render dashboard
5. **Set up custom domain** (optional)

---

## ğŸ“ Support

**Render Documentation:** https://render.com/docs  
**FastAPI Deployment:** https://fastapi.tiangolo.com/deployment/  
**Render Community:** https://community.render.com/  

---

**Made with â¤ï¸ by Group 2: Regressors**
